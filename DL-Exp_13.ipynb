{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance pandas numpy scikit-learn tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUdce-jkp0UL",
        "outputId": "66c54d4c-ec0f-433c-d851-6225ea117738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.57)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.7)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6vMvLdToxW9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Parameters\n",
        "TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'META']\n",
        "WINDOW_SIZE = 20  # e.g., past 20 days\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n"
      ],
      "metadata": {
        "id": "6Y-OMHj6ozyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Download data\n",
        "def download_data(tickers, start=None, end=None):\n",
        "    df = yf.download(tickers, period='2y', auto_adjust=False)\n",
        "    # Flatten column hierarchy\n",
        "    df.columns = ['_'.join(col).strip() for col in df.columns.values]\n",
        "    return df\n",
        "\n",
        "data = download_data(TICKERS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uUvGc_ouEMU",
        "outputId": "6128cfce-7215-445d-9997-015c8036ea62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  3 of 4 completed\n",
            "ERROR:yfinance:\n",
            "4 Failed downloads:\n",
            "ERROR:yfinance:['AAPL', 'GOOGL', 'MSFT', 'META']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
            "[*********************100%***********************]  3 of 4 completed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Columns in DataFrame:\", data.columns.tolist())\n",
        "print(\"TICKERS:\", TICKERS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pyGpC-KuGOm",
        "outputId": "c67e27c3-424d-4c59-ab8d-ab0d90a0a59f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in DataFrame: ['Adj Close_AAPL', 'Adj Close_GOOGL', 'Adj Close_META', 'Adj Close_MSFT', 'Close_AAPL', 'Close_GOOGL', 'Close_META', 'Close_MSFT', 'High_AAPL', 'High_GOOGL', 'High_META', 'High_MSFT', 'Low_AAPL', 'Low_GOOGL', 'Low_META', 'Low_MSFT', 'Open_AAPL', 'Open_GOOGL', 'Open_META', 'Open_MSFT', 'Volume_AAPL', 'Volume_GOOGL', 'Volume_META', 'Volume_MSFT']\n",
            "TICKERS: ['AAPL', 'MSFT', 'GOOGL', 'META']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Feature engineering\n",
        "#   Compute RSI, SMA, VWAP\n",
        "\n",
        "def compute_indicators(df):\n",
        "    result = []\n",
        "    for ticker in TICKERS:\n",
        "        o = df[f'Open_{ticker}']\n",
        "        h = df[f'High_{ticker}']\n",
        "        l = df[f'Low_{ticker}']\n",
        "        c = df[f'Close_{ticker}']\n",
        "        v = df[f'Volume_{ticker}']\n",
        "        # SMA\n",
        "        sma = c.rolling(window=14).mean()\n",
        "        # RSI\n",
        "        delta = c.diff()\n",
        "        gain = delta.clip(lower=0)\n",
        "        loss = -delta.clip(upper=0)\n",
        "        avg_gain = gain.rolling(window=14).mean()\n",
        "        avg_loss = loss.rolling(window=14).mean()\n",
        "        rs = avg_gain / avg_loss\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "        # VWAP\n",
        "        vwap = ( (h + l + c) / 3 * v ).cumsum() / v.cumsum()\n",
        "        # assemble\n",
        "        df_t = pd.DataFrame({\n",
        "            'Open': o,\n",
        "            'High': h,\n",
        "            'Low': l,\n",
        "            'Close': c,\n",
        "            'Volume': v,\n",
        "            'SMA_14': sma,\n",
        "            'RSI_14': rsi,\n",
        "            'VWAP': vwap\n",
        "        })\n",
        "        # Label: next-day movement\n",
        "        df_t['Future_Close'] = df_t['Close'].shift(-1)\n",
        "        df_t['Target'] = (df_t['Future_Close'] > df_t['Close']).astype(int)\n",
        "        df_t.dropna(inplace=True)\n",
        "        # Prefix ticker\n",
        "        df_t.columns = [f'{ticker}_{col}' for col in df_t.columns]\n",
        "        result.append(df_t)\n",
        "    return pd.concat(result, axis=1).dropna()\n",
        "\n",
        "features = compute_indicators(data)\n"
      ],
      "metadata": {
        "id": "98fih-6LuJ9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Prepare sequences\n",
        "def create_sequences(df, window_size):\n",
        "    X, y = [], []\n",
        "    values = df.values\n",
        "    for i in range(len(values) - window_size):\n",
        "        X.append(values[i:i + window_size, :-2])  # all features except Future_Close & Target\n",
        "        y.append(values[i + window_size - 1, -1])  # Target at end of window\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = create_sequences(features, WINDOW_SIZE)\n",
        "\n",
        "# 5. Train/test split & scaling\n",
        "nsamples, ntimesteps, nfeatures = X.shape\n",
        "X = X.reshape(-1, nfeatures)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = X_scaled.reshape(nsamples, ntimesteps, nfeatures)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=False\n",
        ")\n",
        "\n",
        "# 6. Build CNN model\n",
        "model = Sequential([\n",
        "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(WINDOW_SIZE, nfeatures)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.2),\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.2),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# 7. Train\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# 8. Evaluate\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# 9. Save model\n",
        "model.save('cnn_stock_classifier.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "z69n-tkyuMhV",
        "outputId": "796d26ed-8e1d-4658-ef0c-9166c7d42023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-2b44eaa87b4e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 5. Train/test split & scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mnsamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "\n",
        "# 1) Parameters\n",
        "TICKERS     = ['AAPL','MSFT','GOOGL','META']\n",
        "WINDOW_SIZE = 20\n",
        "TEST_SIZE   = 0.2\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# 2) Download & flatten\n",
        "raw = yf.download(TICKERS, period='2y', auto_adjust=False)\n",
        "raw.columns = ['_'.join(col) for col in raw.columns]\n",
        "\n",
        "# 3) Indicators + Target\n",
        "def compute_indicators(df, window=14):\n",
        "    blocks = []\n",
        "    for t in TICKERS:\n",
        "        o = df[f'{t}_Open'];\n",
        "        h = df[f'{t}_High']\n",
        "        l = df[f'{t}_Low'];\n",
        "        c = df[f'{t}_Close']\n",
        "        v = df[f'{t}_Volume']\n",
        "        sma = c.rolling(window).mean()\n",
        "        delta = c.diff()\n",
        "        gain  = delta.clip(lower=0).rolling(window).mean()\n",
        "        loss  = -delta.clip(upper=0).rolling(window).mean()\n",
        "        rs    = gain / (loss + 1e-8)\n",
        "        rsi   = 100 - (100/(1+rs))\n",
        "        tp    = (h + l + c)/3\n",
        "        vwap  = (tp * v).cumsum() / (v.cumsum() + 1e-8)\n",
        "\n",
        "        df_t = pd.DataFrame({\n",
        "            'Open': o, 'High': h, 'Low': l, 'Close': c, 'Volume': v,\n",
        "            'SMA': sma, 'RSI': rsi, 'VWAP': vwap\n",
        "        })\n",
        "        df_t['Target'] = (df_t['Close'].shift(-1) > df_t['Close']).astype(int)\n",
        "        df_t.dropna(inplace=True)\n",
        "        df_t.columns = [f'{t}_{col}' for col in df_t.columns]\n",
        "        blocks.append(df_t)\n",
        "\n",
        "    feat = pd.concat(blocks, axis=1).dropna()\n",
        "    # drop the shifted Future_Close columns if you created them; here we only have one 'Target'\n",
        "    return feat\n",
        "\n",
        "features = compute_indicators(raw)\n",
        "\n",
        "# 4) Sequence creation\n",
        "def create_sequences(df, window_size):\n",
        "    X, y = [], []\n",
        "    vals = df.values\n",
        "    n = len(vals) - window_size\n",
        "    for i in range(n):\n",
        "        X.append(vals[i : i + window_size, :-1])\n",
        "        y.append(vals[i + window_size,  -1])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = create_sequences(features, WINDOW_SIZE)\n",
        "\n",
        "# 5) Scale & split\n",
        "n_s, n_t, n_f = X.shape\n",
        "X_flat = X.reshape(-1, n_f)\n",
        "scaler = StandardScaler().fit(X_flat)\n",
        "X_scaled = scaler.transform(X_flat).reshape(n_s, n_t, n_f)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=TEST_SIZE, shuffle=False, random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "# 6) Build & compile CNN\n",
        "model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(WINDOW_SIZE, n_f)),\n",
        "    BatchNormalization(), MaxPooling1D(2), Dropout(0.2),\n",
        "    Conv1D(64, 3, activation='relu'), BatchNormalization(),\n",
        "    MaxPooling1D(2), Dropout(0.2),\n",
        "    Flatten(), Dense(64, activation='relu'), Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile('adam', 'binary_crossentropy', ['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# 7) Train\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=20, batch_size=32\n",
        ")\n",
        "\n",
        "# 8) Eval & save\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}, Test Acc: {acc:.4f}\")\n",
        "model.save('cnn_stock_classifier.h5')\n"
      ],
      "metadata": {
        "id": "_E4Q96P6uRM_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "outputId": "35a8500f-c0be-48c8-f282-3a6da2d314d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  4 of 4 completed\n",
            "ERROR:yfinance:\n",
            "4 Failed downloads:\n",
            "ERROR:yfinance:['GOOGL', 'AAPL', 'META', 'MSFT']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'AAPL_Open'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'AAPL_Open'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a6cc1258492b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_indicators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# 4) Sequence creation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-a6cc1258492b>\u001b[0m in \u001b[0;36mcompute_indicators\u001b[0;34m(df, window)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTICKERS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{t}_Open'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{t}_High'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{t}_Low'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'AAPL_Open'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Part A: Tokenization and Stemming using NLTK\n",
        "\n",
        "def tokenize_and_stem(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text and returns a list of stemmed tokens.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "# Part B: Lemmatization and Stopwords Removal using NLTK\n",
        "\n",
        "def lemmatize_and_remove_stopwords_nltk(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the text, removes English stopwords, and returns a list of lemmas.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [lemmatizer.lemmatize(token.lower())\n",
        "            for token in tokens\n",
        "            if token.isalpha() and token.lower() not in stop_words]\n",
        "\n",
        "# Part C: Lemmatization and Stopwords Removal using spaCy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def lemmatize_and_remove_stopwords_spacy(text):\n",
        "    \"\"\"\n",
        "    Processes the text with spaCy, removes stopwords and punctuation,\n",
        "    and returns a list of token lemmas.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc\n",
        "            if not token.is_stop and not token.is_punct]\n",
        "\n",
        "# Part D: Bag-of-Words using CountVectorizer\n",
        "\n",
        "def bag_of_words_count(texts, max_features=None):\n",
        "    \"\"\"\n",
        "    Fits a CountVectorizer on the list of texts and returns\n",
        "    the feature matrix and vectorizer instance.\n",
        "    \"\"\"\n",
        "    vectorizer = CountVectorizer(max_features=max_features)\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return X, vectorizer\n",
        "\n",
        "# Part E: Bag-of-n-grams using CountVectorizer\n",
        "\n",
        "def bag_of_ngrams_count(texts, n_range=(1, 2), max_features=None):\n",
        "    \"\"\"\n",
        "    Fits a CountVectorizer that captures n-grams in the specified range\n",
        "    and returns the feature matrix and vectorizer instance.\n",
        "    \"\"\"\n",
        "    vectorizer = CountVectorizer(ngram_range=n_range, max_features=max_features)\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return X, vectorizer\n",
        "\n",
        "# Part F: TF-IDF Bag-of-Words using TfidfVectorizer\n",
        "\n",
        "def bag_of_words_tfidf(texts, max_features=None):\n",
        "    \"\"\"\n",
        "    Fits a TfidfVectorizer on the list of texts and returns\n",
        "    the TF-IDF feature matrix and vectorizer instance.\n",
        "    \"\"\"\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return X, vectorizer\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    sample_texts = [\n",
        "        \"Natural Language Processing (NLP) is fascinating.\",\n",
        "        \"Tokenization, stemming, and lemmatization are crucial steps.\"\n",
        "    ]\n",
        "\n",
        "    # Preprocessing examples\n",
        "    print(\"[NLTK] Tokenize & Stem:\", tokenize_and_stem(sample_texts[0]))\n",
        "    print(\"[NLTK] Lemmatize & Remove Stopwords:\", lemmatize_and_remove_stopwords_nltk(sample_texts[1]))\n",
        "    print(\"[spaCy] Lemmatize & Remove Stopwords:\", lemmatize_and_remove_stopwords_spacy(sample_texts[1]))\n",
        "\n",
        "    # Vectorization examples\n",
        "    X_bow, bow_vec = bag_of_words_count(sample_texts)\n",
        "    print(\"\\nBag-of-Words feature names:\", bow_vec.get_feature_names_out())\n",
        "\n",
        "    X_ngrams, ngram_vec = bag_of_ngrams_count(sample_texts, n_range=(1, 3))\n",
        "    print(\"\\nBag-of-n-grams (1-3) feature names:\", ngram_vec.get_feature_names_out())\n",
        "\n",
        "    X_tfidf, tfidf_vec = bag_of_words_tfidf(sample_texts)\n",
        "    print(\"\\nTF-IDF feature names:\", tfidf_vec.get_feature_names_out())\n",
        "\n",
        "if __name__== '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "wMbB5ONyw80T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "outputId": "21a4cd96-d7bf-4b0b-9735-fa50d0c0eea6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-36472be5eff6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-36472be5eff6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Preprocessing examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[NLTK] Tokenize & Stem:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_and_stem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[NLTK] Lemmatize & Remove Stopwords:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatize_and_remove_stopwords_nltk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[spaCy] Lemmatize & Remove Stopwords:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatize_and_remove_stopwords_spacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-36472be5eff6>\u001b[0m in \u001b[0;36mtokenize_and_stem\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mTokenizes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mstemmed\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# Download the missing 'punkt_tab' resource\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# Part A: Tokenization and Stemming using NLTK\n",
        "\n",
        "def tokenize_and_stem(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text and returns a list of stemmed tokens.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "# Part B: Lemmatization and Stopwords Removal using NLTK\n",
        "\n",
        "def lemmatize_and_remove_stopwords_nltk(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the text, removes English stopwords, and returns a list of lemmas.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [lemmatizer.lemmatize(token.lower())\n",
        "            for token in tokens\n",
        "            if token.isalpha() and token.lower() not in stop_words]\n",
        "\n",
        "# Part C: Lemmatization and Stopwords Removal using spaCy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def lemmatize_and_remove_stopwords_spacy(text):\n",
        "    \"\"\"\n",
        "    Processes the text with spaCy, removes stopwords and punctuation,\n",
        "    and returns a list of token lemmas.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc\n",
        "            if not token.is_stop and not token.is_punct]\n",
        "\n",
        "# Part D: Bag-of-Words using CountVectorizer\n",
        "\n",
        "def bag_of_words_count(texts, max_features=None):\n",
        "    \"\"\"\n",
        "    Fits a CountVectorizer on the list of texts and returns\n",
        "    the feature matrix and vectorizer instance.\n",
        "    \"\"\"\n",
        "    vectorizer = CountVectorizer(max_features=max_features)\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return X, vectorizer\n",
        "\n",
        "# Part E: Bag-of-n-grams using CountVectorizer\n",
        "\n",
        "def bag_of_ngrams_count(texts, n_range=(1, 2), max_features=None):\n",
        "    \"\"\"\n",
        "    Fits a CountVectorizer that captures n-grams in the specified range\n",
        "    and returns the feature matrix and vectorizer instance.\n",
        "    \"\"\"\n",
        "    vectorizer = CountVectorizer(ngram_range=n_range, max_features=max_features)\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return X, vectorizer\n",
        "\n",
        "# Part F: TF-IDF Bag-of-Words using TfidfVectorizer\n",
        "\n",
        "def bag_of_words_tfidf(texts, max_features=None):\n",
        "    \"\"\"\n",
        "    Fits a TfidfVectorizer on the list of texts and returns\n",
        "    the TF-IDF feature matrix and vectorizer instance.\n",
        "    \"\"\"\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return X, vectorizer\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    sample_texts = [\n",
        "        \"Natural Language Processing (NLP) is fascinating.\",\n",
        "        \"Tokenization, stemming, and lemmatization are crucial steps.\"\n",
        "    ]\n",
        "\n",
        "    # Preprocessing examples\n",
        "    print(\"[NLTK] Tokenize & Stem:\", tokenize_and_stem(sample_texts[0]))\n",
        "    print(\"[NLTK] Lemmatize & Remove Stopwords:\", lemmatize_and_remove_stopwords_nltk(sample_texts[1]))\n",
        "    print(\"[spaCy] Lemmatize & Remove Stopwords:\", lemmatize_and_remove_stopwords_spacy(sample_texts[1]))\n",
        "\n",
        "    # Vectorization examples\n",
        "    X_bow, bow_vec = bag_of_words_count(sample_texts)\n",
        "    print(\"\\nBag-of-Words feature names:\", bow_vec.get_feature_names_out())\n",
        "\n",
        "    X_ngrams, ngram_vec = bag_of_ngrams_count(sample_texts, n_range=(1, 3))\n",
        "    print(\"\\nBag-of-n-grams (1-3) feature names:\", ngram_vec.get_feature_names_out())\n",
        "\n",
        "    X_tfidf, tfidf_vec = bag_of_words_tfidf(sample_texts)\n",
        "    print(\"\\nTF-IDF feature names:\", tfidf_vec.get_feature_names_out())\n",
        "\n",
        "if __name__== '__main__':\n",
        "    main()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvcJ7BMPb3Cg",
        "outputId": "4ffb0966-de53-4176-e94c-f5dd5e311955"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NLTK] Tokenize & Stem: ['natur', 'languag', 'process', '(', 'nlp', ')', 'is', 'fascin', '.']\n",
            "[NLTK] Lemmatize & Remove Stopwords: ['tokenization', 'stemming', 'lemmatization', 'crucial', 'step']\n",
            "[spaCy] Lemmatize & Remove Stopwords: ['Tokenization', 'stemming', 'lemmatization', 'crucial', 'step']\n",
            "\n",
            "Bag-of-Words feature names: ['and' 'are' 'crucial' 'fascinating' 'is' 'language' 'lemmatization'\n",
            " 'natural' 'nlp' 'processing' 'stemming' 'steps' 'tokenization']\n",
            "\n",
            "Bag-of-n-grams (1-3) feature names: ['and' 'and lemmatization' 'and lemmatization are' 'are' 'are crucial'\n",
            " 'are crucial steps' 'crucial' 'crucial steps' 'fascinating' 'is'\n",
            " 'is fascinating' 'language' 'language processing'\n",
            " 'language processing nlp' 'lemmatization' 'lemmatization are'\n",
            " 'lemmatization are crucial' 'natural' 'natural language'\n",
            " 'natural language processing' 'nlp' 'nlp is' 'nlp is fascinating'\n",
            " 'processing' 'processing nlp' 'processing nlp is' 'stemming'\n",
            " 'stemming and' 'stemming and lemmatization' 'steps' 'tokenization'\n",
            " 'tokenization stemming' 'tokenization stemming and']\n",
            "\n",
            "TF-IDF feature names: ['and' 'are' 'crucial' 'fascinating' 'is' 'language' 'lemmatization'\n",
            " 'natural' 'nlp' 'processing' 'stemming' 'steps' 'tokenization']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# Download the 'punkt_tab' data package\n",
        "nltk.download('punkt_tab') # This line is added to download the missing data\n",
        "\n",
        "# Part A: Tokenization and Stemming using NLTK\n",
        "\n",
        "def tokenize_and_stem(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text and returns a list of stemmed tokens.\n",
        "    \"\"\"\n",
        "    # Tokenize text into words\n",
        "    tokens = word_tokenize(text)\n",
        "    # Initialize the Porter Stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    # Stem each token\n",
        "    stemmed = [stemmer.stem(token) for token in tokens]\n",
        "    return stemmed\n",
        "\n",
        "# Part B: Lemmatization and Stopwords Removal using NLTK\n",
        "\n",
        "def lemmatize_and_remove_stopwords_nltk(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the text, removes English stopwords, and returns a list of lemmas.\n",
        "    \"\"\"\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Initialize lemmatizer and stopwords list\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Lowercase, remove stopwords, and lemmatize\n",
        "    lemmas = [lemmatizer.lemmatize(token.lower())\n",
        "              for token in tokens\n",
        "              if token.isalpha() and token.lower() not in stop_words]\n",
        "    return lemmas\n",
        "\n",
        "# Part C: Lemmatization and Stopwords Removal using spaCy\n",
        "\n",
        "# Load the English model. Make sure to install with:\n",
        "#   python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def lemmatize_and_remove_stopwords_spacy(text):\n",
        "    \"\"\"\n",
        "    Processes the text with spaCy, removes stopwords and punctuation,\n",
        "    and returns a list of token lemmas.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    cleaned = [token.lemma_ for token in doc\n",
        "               if not token.is_stop and not token.is_punct]\n",
        "    return cleaned\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    sample_text = \"Natural Language Processing (NLP) is a fascinating field of AI!\"\n",
        "    print(\"Original Text:\", sample_text)\n",
        "    print(\"\\n[NLTK] Tokenize & Stem:\", tokenize_and_stem(sample_text))\n",
        "    print(\"\\n[NLTK] Lemmatize & Remove Stopwords:\", lemmatize_and_remove_stopwords_nltk(sample_text))\n",
        "    print(\"\\n[spaCy] Lemmatize & Remove Stopwords:\", lemmatize_and_remove_stopwords_spacy(sample_text))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzRHQpQZatx_",
        "outputId": "ee1d7d2b-aa05-47c3-e3bd-4cc9c6d0dee4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Natural Language Processing (NLP) is a fascinating field of AI!\n",
            "\n",
            "[NLTK] Tokenize & Stem: ['natur', 'languag', 'process', '(', 'nlp', ')', 'is', 'a', 'fascin', 'field', 'of', 'ai', '!']\n",
            "\n",
            "[NLTK] Lemmatize & Remove Stopwords: ['natural', 'language', 'processing', 'nlp', 'fascinating', 'field', 'ai']\n",
            "\n",
            "[spaCy] Lemmatize & Remove Stopwords: ['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'AI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "si1lKHVwdUqn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}